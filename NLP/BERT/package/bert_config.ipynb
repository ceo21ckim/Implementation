{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "\n",
    "create : 2021-12-03, 13:44\n",
    "\n",
    "modify : 2021-12-06, 12:46\n",
    "\n",
    "author : KIM DONG EON\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version :  1.9.1+cpu\n",
      "transformers version :  4.10.2\n",
      "pandas version :  1.2.3\n",
      "numpy version :  1.19.5\n"
     ]
    }
   ],
   "source": [
    "_version = [torch, transformers, pd, np]\n",
    "\n",
    "for ver in _version:\n",
    "    print(str(ver).split(\"'\")[1], 'version : ', ver.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertConfig\n",
    "- vocab_size : int, defaults = 30522, Vocabluary size of the BERT. BertModel, TFBertModel 을 호출할 때 inputs_ids 로 표현할 수 있는 토큰의 수를 의미함.\n",
    "- hidden_size : int, defaults = 768, Dimensionality of the encoder layers and the pooler layer.\n",
    "- num_hidden_layers : int, defaults = 12, Number of hidden layers in the Transformer encoder.\n",
    "- num_attention_heads : int, defaults = 12, Number of attention heads for each attention layer in the Transformer encoder. [BERT_based_uncased]\n",
    "- intermediate_size : int, defaults = 3072, Dimensionality of the 'intermediate' layer in the Transformer encoder.\n",
    "- hidden_act : str or Callable, defaults = 'gelu', the non-linear activation function in the encoder and pooler. [gelu, relu, silu, gelu_new] are supported.\n",
    "- hidden_dropout_prob : float, defaults = 0.1, The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "- attention_probs_dropout_prob : float, defaults = 0.1, The dropout ratio for the attention probabilities.\n",
    "- max_position_embeddings : int, defaults = 512, The maximum sequence length that this model might ever be used with. Typically set this to something large just in case.(e.g., 512, 1024, 2048)\n",
    "- type_vocab_size : int, defaults = 2, The vocabulary size of the token_type_ids passed when calling BertModel or TFBertModel.\n",
    "- initializer_range : float, defaults = 0.02, The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "- layer_norm_eps : float, defaults = 1e-12 : The epsilon used by the layer normalization layers.\n",
    "- position_embedding_type : str, defaults = 'absolute', Type of position embedding. Choose 'absolute', 'relative_key', 'relative_key_query'\n",
    "- use_cache : bool, defaults = True, Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if if config.is_decoder=True.\n",
    "- classifier_dropout : float, The dropout ratio for the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.10.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertConfig(\n",
    "    vocab_size=30522, \n",
    "    hidden_size=768, \n",
    "    num_hidden_layers=12, # bert-base-uncased : 12, BERT-large : 24\n",
    "    num_attention_heads=12, # BERT-large : 16\n",
    "    intermediate_size=3072, \n",
    "    hidden_act = 'gelu', # relu, gelu, silu, gelu_new\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1, \n",
    "    max_position_embeddings=512, # 512, 1024, 2048\n",
    "    type_vocab_size=2, \n",
    "    initializer_range=0.02, # 0.01, 0.001, deviation\n",
    "    layer_norm_eps=1e-12, # normalization layers\n",
    "    position_embedding_type='absolute', # absolute, relative_key, relative_key_query\n",
    "    use_cache=True, # boolean\n",
    "    # classifier_dropout= null # float\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig()\n",
    "\n",
    "# initializing a model from the bert-base-uncased style configuration\n",
    "model = BertModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a957a1fea41707a244f264f295721eb09ec21d5be06b513cdb5f37b29d60cc5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('geo': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
