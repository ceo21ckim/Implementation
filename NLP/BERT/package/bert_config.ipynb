{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "\n",
    "create : 2021-12-03, 13:44\n",
    "\n",
    "modify : 2021-12-06, 12:46\n",
    "\n",
    "author : KIM DONG EON\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version :  1.9.1+cpu\n",
      "transformers version :  4.10.2\n",
      "pandas version :  1.2.3\n",
      "numpy version :  1.19.5\n"
     ]
    }
   ],
   "source": [
    "_version = [torch, transformers, pd, np]\n",
    "\n",
    "for ver in _version:\n",
    "    print(str(ver).split(\"'\")[1], 'version : ', ver.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertConfig\n",
    "- vocab_size : int, defaults = 30522, Vocabluary size of the BERT. BertModel, TFBertModel 을 호출할 때 inputs_ids 로 표현할 수 있는 토큰의 수를 의미함.\n",
    "- hidden_size : int, defaults = 768, Dimensionality of the encoder layers and the pooler layer.\n",
    "- num_hidden_layers : int, defaults = 12, Number of hidden layers in the Transformer encoder.\n",
    "- num_attention_heads : int, defaults = 12, Number of attention heads for each attention layer in the Transformer encoder. [BERT_based_uncased]\n",
    "- intermediate_size : int, defaults = 3072, Dimensionality of the 'intermediate' layer in the Transformer encoder.\n",
    "- hidden_act : str or Callable, defaults = 'gelu', the non-linear activation function in the encoder and pooler. [gelu, relu, silu, gelu_new] are supported.\n",
    "- hidden_dropout_prob : float, defaults = 0.1, The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "- attention_probs_dropout_prob : float, defaults = 0.1, The dropout ratio for the attention probabilities.\n",
    "- max_position_embeddings : int, defaults = 512, The maximum sequence length that this model might ever be used with. Typically set this to something large just in case.(e.g., 512, 1024, 2048)\n",
    "- type_vocab_size : int, defaults = 2, The vocabulary size of the token_type_ids passed when calling BertModel or TFBertModel.\n",
    "- initializer_range : float, defaults = 0.02, The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "- layer_norm_eps : float, defaults = 1e-12 : The epsilon used by the layer normalization layers.\n",
    "- position_embedding_type : str, defaults = 'absolute', Type of position embedding. Choose 'absolute', 'relative_key', 'relative_key_query'\n",
    "- use_cache : bool, defaults = True, Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if if config.is_decoder=True.\n",
    "- classifier_dropout : float, The dropout ratio for the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.10.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertConfig(\n",
    "    vocab_size=30522, \n",
    "    hidden_size=768, \n",
    "    num_hidden_layers=12, # bert-base-uncased : 12, BERT-large : 24\n",
    "    num_attention_heads=12, # BERT-large : 16\n",
    "    intermediate_size=3072, \n",
    "    hidden_act = 'gelu', # relu, gelu, silu, gelu_new\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1, \n",
    "    max_position_embeddings=512, # 512, 1024, 2048\n",
    "    type_vocab_size=2, \n",
    "    initializer_range=0.02, # 0.01, 0.001, deviation\n",
    "    layer_norm_eps=1e-12, # normalization layers\n",
    "    position_embedding_type='absolute', # absolute, relative_key, relative_key_query\n",
    "    use_cache=True, # boolean\n",
    "    # classifier_dropout= null # float\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig()\n",
    "\n",
    "# initializing a model from the bert-base-uncased style configuration\n",
    "model = BertModel(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertTokenizer\n",
    "- Construct a BERT tokenizer. Based on WordPiece.\n",
    "- do_lower_case : Whether or not to lowercase the input when tokenizing.\n",
    "- do_basic_tokenize : Whether or not to do basic tokenizaiton befor WordPiece.\n",
    "- never_split : Collection of tokens which will never be split during tokenization. Only has an effect when do_basic_tokenize = True\n",
    "- unk_token : The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this token instead.\n",
    "- sep_token : The separator token. \n",
    "- pad_token : The token used for padding.\n",
    "- cls_token : The classifier token which is used when doing sequence classification.\n",
    "- mask_token : The token used for masking values.\n",
    "- tokenize_chinese_char : Whether or not to tokenize Chinese characters.\n",
    "- strip_accents : This should likely be deactivated for Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer('file.path',\n",
    "    # vocab_file= 'str' # File containing the vocabulary\n",
    "    do_lower_case=True, # uncased\n",
    "    do_basic_tokenize=True, \n",
    "    never_split=None, \n",
    "    unk_token='[UNK]', \n",
    "    sep_token='[SEP]', \n",
    "    pad_token='[PAD]',\n",
    "    cls_token='[CLS]', \n",
    "    mask_token='[MASK]', \n",
    "    tokenize_chinese_chars=True, \n",
    "    strip_accents=False # This should likely be deactivated for Japanese\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertTokenizerFast\n",
    "- Construct a 'fast' BERT tokenizer (backed by HuggingFace's tokenizers library). Based on WordPiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fast = BertTokenizerFast(\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 281kB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 14.0kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:01<00:00, 457kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 71.6kB/s]\n",
      "Downloading: 100%|██████████| 440M/440M [00:37<00:00, 11.7MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig()\n",
    "\n",
    "# initializing a model from the bert-base-uncased style configuration\n",
    "model = BertModel(configuration)\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7632,  1010,  2026,  2171,  2003, 11947, 10242,  5035,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer('hi, my name is DONGEON KIM', return_tensors= 'pt')\n",
    "inputs # input_ids, token_type_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('hi, my name is DONGEON KIM', return_tensors= 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_logits = outputs.prediction_logits\n",
    "seq_relationship_logits = outputs.seq_relationship_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(seq_relationship_logits, 1)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a957a1fea41707a244f264f295721eb09ec21d5be06b513cdb5f37b29d60cc5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('geo': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
